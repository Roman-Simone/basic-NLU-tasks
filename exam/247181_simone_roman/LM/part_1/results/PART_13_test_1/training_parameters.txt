PART_13

Hidden Size: 200
Embedding Size: 300
Learning Rate: 0.0015
Clip: 5
Vocabulary Length: 10001
Number of Epochs: 12
Best Test PPL: 123.89672718776508
Batch Size Train: 64
Batch Size Dev: 128
Batch Size Test: 128
Optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0015
    maximize: False
    weight_decay: 0.01
)
Model: LM_LSTM_DROP(
  (embedding): Embedding(10001, 300, padding_idx=0)
  (emb_dropout): Dropout(p=0.1, inplace=False)
  (lstm): LSTM(300, 200, batch_first=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (output): Linear(in_features=200, out_features=10001, bias=True)
)
